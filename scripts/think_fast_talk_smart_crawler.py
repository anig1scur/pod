import os
import json
import requests
from bs4 import BeautifulSoup
from utils import XML_HEADERS


ART19_HEADERS = {
    "Content-Type": "application/json",
    "Accept": "application/json",
    "Accept-Encoding": "gzip, deflate, br, zstd",
}

HOST = "https://www.gsb.stanford.edu"
TFTS_URL = (
    "https://www.gsb.stanford.edu/business-podcasts/think-fast-talk-smart-podcast"
)
OUTPUT_FOLDER = "./public/assets/tfts/scripts"
COVER = "https://www.gsb.stanford.edu/sites/default/files/styles/1630x_variable/public/tfts-key_logo_21.jpg.webp?itok=S-EaSW2x"
RSS_ART19 = "https://rss.art19.com/episodes/"


def extract_all_episode_urls():
    response = requests.get(TFTS_URL, headers=XML_HEADERS, timeout=20)
    if response.status_code == 200:
        classname = "views-field views-field-title"
        soup = BeautifulSoup(response.text, "html.parser")
        titles = soup.find_all("h3", {"class": classname})
        links = [HOST + title.find("a")["href"] for title in titles]
        return links
    else:
        print(f"Failed to retrieve the page. Status code: {response.status_code}")
        return []


def extract_transcript_from_page(soup):
    note = "Note: Transcripts are generated by machine and lightly edited by humans. They may contain errors."
    paragraphs = soup.find("div", {"class": "description"}).find_all("p")
    transcript = []
    authors = []
    cur_author = None
    for paragraph in paragraphs:
        if paragraph.text.strip() == note:
            continue
        if paragraph.find("strong"):
            author = paragraph.find("strong").text.strip().strip(":")
            text = (
                paragraph.text.replace(author, "", 1)
                .strip(":")
                .strip()
                .replace("\n", " ")
            )
            author = author.replace("\u00a0", "'")
            transcript.append({"author": author, "text": text})
            cur_author = author
            if author not in authors:
                authors.append(author)
        else:
            text = paragraph.text.replace("\n", " ")
            if not transcript:
                continue
            transcript.append({"author": cur_author, "text": text})
    return authors, transcript


def extract_description_from_page(soup):
    description_parent = soup.find(
        "div", {"class": "textblock__content textblock__content--unstyled p"}
    )
    description = [
        p.text.strip().replace("\u00a0", " ").replace("\u2019", "'")
        for p in description_parent.find_all("p")[:2]
    ]
    # sometimes description and transcript mixed together, so pick first 2 paragraphs.
    # https://www.gsb.stanford.edu/insights/science-influence-how-persuade-others-hold-their-attention
    return [d for d in description if d]


def get_id_from_page(soup):
    art19_target = soup.find("meta", {"name": "twitter:player"}) or soup.find(
        "meta", {"name": "twitter:url"}
    )
    return art19_target["content"].split("/")[-1].split("?")[0]


def get_audio_url_from_art19(id):
    response = requests.get(RSS_ART19 + id, headers=ART19_HEADERS, timeout=10)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, "html.parser")
        data = json.loads(soup.text)
        print(data["content"])
        audio_url = data["content"]["media"]["mp3"]["url"]
        return audio_url
    else:
        print(f"Failed to retrieve the page. Status code: {response.status_code}")
        return None


def extract_and_save_episode_data(url):
    file_name = url.split("/")[-1]
    file_path = os.path.join(OUTPUT_FOLDER, f"{file_name}.json")

    if os.path.exists(file_path):
        # print(f"{file_name} already exists, skipping extraction.")
        return

    response = requests.get(url, headers=HEADERS)
    soup = BeautifulSoup(response.text, "html.parser")
    print(f"Extracting data from {url}")
    id = get_id_from_page(soup)
    title = soup.find("h1").text.strip("\n").strip()
    authors, transcript = extract_transcript_from_page(soup)
    description = extract_description_from_page(soup)
    audio_url = get_audio_url_from_art19(id)
    data = {
        "title": title,
        "url": url,
        "intro": description,
        "transcript": transcript,
        "audio": audio_url,
        "img": COVER,
        "authors": authors,
    }
    with open(file_path, "w+") as f:
        f.write(json.dumps(data, indent=2))


def main():
    episode_urls = extract_all_episode_urls()
    for url in episode_urls:
        extract_and_save_episode_data(url)


if __name__ == "__main__":
    main()
